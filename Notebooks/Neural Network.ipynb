{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diego\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\diego\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# DataSet de départ\n",
    "data = pd.read_csv(\"data.csv\", encoding=\"iso-8859-1\")\n",
    "\n",
    "# DataSet des Top Hit (jusqu'à 2017)\n",
    "top_songs = pd.read_csv(\"top_data.csv\", encoding=\"iso-8859-1\")\n",
    "\n",
    "# On filtre donc notre DataSet de départ pour conserver seuelement les musiques avec date > 2017\n",
    "data_2017 = data[data[\"year\"]<2017]\n",
    "\n",
    "\n",
    "# On recherche l'existence des musiques dans le DataSet des Hit Songs pour créer une nouvelle variable de succès \n",
    "data_2017[\"top\"] = data_2017[\"name\"].isin(top_songs[\"title\"])\n",
    "data_2017[\"top\"] = data_2017[\"top\"].astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014    2000\n",
      "2013    2000\n",
      "2012    2000\n",
      "2011    2000\n",
      "2015    1931\n",
      "Name: year, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Création de datasets en fonction des années\n",
    "def split_df_by_year(df, range_max, year):\n",
    "    \"\"\"\n",
    "        df : dataframe d'entrée\n",
    "        step : range maximum d'annees que l'on veut pout notre dataset\n",
    "        year : annnee de sortie de la musique à preédire\n",
    "        Renvoie le dataframe respectant le range_max et l'annee\n",
    "    \"\"\"\n",
    "\n",
    "    mask = (df['year'] <= year) & (df['year'] > year - range_max)\n",
    "    current_df = df.loc[mask]\n",
    "        \n",
    "        \n",
    "    return current_df\n",
    "         \n",
    "df = split_df_by_year(data_2017, 5, 2015)\n",
    "print(df[\"year\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_2017.columns)\n",
    "data_2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install keras_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diego\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\diego\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\diego\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\diego\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n",
      "126/126 [==============================] - 0s 2ms/step - loss: 7.6981 - accuracy: 0.6320\n",
      "126/126 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.6320\n",
      "Accuracy: 63.20\n"
     ]
    }
   ],
   "source": [
    "### Predict Top Hit ###\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#input_data = data_2017\n",
    "input_data = df\n",
    "input_data[\"duration_ms\"] = input_data[\"duration_ms\"].astype(float)\n",
    "input_data[\"explicit\"] = input_data[\"explicit\"].astype(float)\n",
    "input_data[\"key\"] = input_data[\"key\"].astype(float)\n",
    "input_data[\"mode\"] = input_data[\"mode\"].astype(float)\n",
    "\n",
    "# undersampling\n",
    "#df_ok = input_data[input_data['top'] == 1]\n",
    "#df_ko = input_data.sample(df_ok.shape[0])\n",
    "#input_data = pd.concat([df_ok, df_ko])\n",
    "\n",
    "# Sample to avoid oversampling\n",
    "df_ok = input_data[input_data['top'] == 1]\n",
    "df_ko = input_data.sample(df_ok.shape[0])\n",
    "input_data = pd.concat([df_ok, df_ko])\n",
    "\n",
    "#print(input_data.shape)\n",
    "\n",
    "# Preprocessing des datas\n",
    "train_data, test_data = train_test_split(input_data, test_size=0.2)\n",
    "\n",
    "#x_train = train_data.drop([\"popularity\", \"id\", \"release_date\", \"name\", \"artists\", \"instrumentalness\",\n",
    "                          # \"key\", \"tempo\", \"loudness\", \"year\", \"duration_ms\", \"explicit\", \"mode\"], axis=1)\n",
    "#y_train = x_train.pop('top')\n",
    "\n",
    "#x_test = test_data.drop([\"popularity\", \"id\", \"release_date\", \"name\", \"artists\", \"key\", \"instrumentalness\",\n",
    "                        # \"tempo\", \"loudness\", \"year\", \"duration_ms\", \"explicit\", \"mode\"], axis=1)\n",
    "#y_test = x_test.pop('top')\n",
    "\n",
    "\n",
    "x_train = train_data['acousticness']\n",
    "y_train = train_data['top']\n",
    "\n",
    "x_test = train_data['acousticness']\n",
    "y_test = train_data['top']\n",
    "\n",
    "\n",
    "# Undersampling\n",
    "#rus = RandomUnderSampler()\n",
    "#x_train, y_train = rus.fit_sample(x_train, y_train)\n",
    "\n",
    "# Oversampling\n",
    "#sm = SMOTE()\n",
    "#x_train, y_train = sm.fit_resample(x_train, y_train)\n",
    "#print(x_train.shape)\n",
    "\n",
    "\n",
    "# List out variable types  (pas utilse ici mais on sait jamais)\n",
    "data_type_dict = {'numerical': ['instrumentalness', 'key', 'liveness', 'loudness', 'speechiness', \"instrumentalness\",\n",
    "                                'tempo', 'valence', 'acousticness', 'danceability', 'duration_ms', 'energy'],\n",
    "                  'categorical': ['mode', 'year', 'explicit'],\n",
    "                  'text': ['name', 'artists']}\n",
    "output_var = 'top'\n",
    "\n",
    "\n",
    "#sc = StandardScaler()\n",
    "#sc.fit(x_train)\n",
    "#x_train = sc.transform(x_train)\n",
    "#x_test = sc.transform(x_test)\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "test_data = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the model\n",
    "model = keras.Sequential()\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "tf.keras.Input(shape=1,)\n",
    "#model.add(Dense(6, activation='sigmoid'))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(Dense(2))\n",
    "#model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "#lrate =0.01\n",
    "#model.compile(loss='categorical_crossentropy',\n",
    "#          optimizer=keras.optimizers.SGD(learning_rate= lrate),\n",
    "#          metrics =['accuracy'])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer=keras.optimizers.SGD(learning_rate= 0.1), \n",
    "              metrics=['accuracy']\n",
    "             )\n",
    "\n",
    "\n",
    "#[print(i.shape, i.dtype) for i in model.inputs]\n",
    "#[print(o.shape, o.dtype) for o in model.outputs]\n",
    "#[print(l.name, l.input_shape, l.dtype) for l in model.layers]\n",
    "#model.fit(x_train, y_train, batch_size=32, epochs=10, validation_data=(x_test, y_test))\n",
    "#model.fit(x_train, y_train, validation_data=(x_test, y_test))\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "prediction = model.predict_classes(x_test)\n",
    "#print(prediction)\n",
    "#prediction = model.predict_proba(x_test)\n",
    "\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "y_test = y_test.reset_index()\n",
    "del y_test['index']\n",
    "y_test = y_test.to_numpy()\n",
    "print(y_test)\n",
    "print(y_test.shape)\n",
    "print(prediction.shape)\n",
    "print(prediction)\n",
    "#fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test.to_numpy(), prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0, 1473],\n",
       "       [   0, 2530]], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#print(len(prediction))\n",
    "#print(y_test)\n",
    "#print(prediction)\n",
    "#print(y_test)\n",
    "cm1 = confusion_matrix(y_test,prediction)\n",
    "cm1\n",
    "#print(train_data)\n",
    "#for x, y in train_data:\n",
    "#    print(\"x\", x, \"y\", y)\n",
    "#x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "input_data = data_2017\n",
    "input_data[\"duration_ms\"] = input_data[\"duration_ms\"].astype(float)\n",
    "input_data[\"explicit\"] = input_data[\"explicit\"].astype(float)\n",
    "input_data[\"key\"] = input_data[\"key\"].astype(float)\n",
    "input_data[\"mode\"] = input_data[\"mode\"].astype(float)\n",
    "\n",
    "# Preprocessing des datas\n",
    "train_data, test_data = train_test_split(input_data, test_size=0.2)\n",
    "\n",
    "x_train = train_data.drop([\"popularity\", \"id\", \"release_date\", \"name\", \"artists\"], axis=1)\n",
    "y_train = x_train.pop('top')\n",
    "\n",
    "x_test = test_data.drop([\"popularity\", \"id\", \"release_date\", \"name\", \"artists\"], axis=1)\n",
    "y_test = x_test.pop('top')\n",
    "\n",
    "#y_train = np.asarray(train_labels).astype('float32').reshape((-1,1))\n",
    "#y_test = np.asarray(test_labels).astype('float32').reshape((-1,1))\n",
    "\n",
    "# List out variable types  (pas utilse ici mais on sait jamais)\n",
    "data_type_dict = {'numerical': ['instrumentalness', 'key', 'liveness', 'loudness', 'speechiness', \n",
    "                                'tempo', 'valence', 'acousticness', 'danceability', 'duration_ms', 'energy'],\n",
    "                  'categorical': ['mode', 'year', 'explicit'],\n",
    "                  'text': ['name', 'artists']}\n",
    "output_var = 'top'\n",
    "\n",
    "#train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "test_data = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "\n",
    "\n",
    "print(train_data)\n",
    "print(type(train_data))\n",
    "\n",
    "\n",
    "# Define the model\n",
    "model = keras.Sequential()\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "tf.keras.Input(shape=14,)\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(2, activation='relu'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#[print(i.shape, i.dtype) for i in model.inputs]\n",
    "#[print(o.shape, o.dtype) for o in model.outputs]\n",
    "#[print(l.name, l.input_shape, l.dtype) for l in model.layers]\n",
    "model.fit(x_train, y_train, epochs=1, validation_data=(x_test, y_test))\n",
    "\n",
    "#_, accuracy = model.evaluate(X, y)\n",
    "#print('Accuracy: %.2f' % (accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
